\section{Introduction} 
\label{sec:introduction}

Automatic Music Transcription (AMT) involves automatically generating a symbolic representation of an acoustic musical signal \cite{Benetos2013b}. AMT is considered to be a fundamental topic in the field of music information retrieval (MIR) and has numerous applications in related fields in music technology, such as interactive music applications and computational musicology. %Typically, the output of an AMT system is a \textit{piano-roll} representation, which is a two-dimensional matrix representation of a musical piece where the X-axis represents time quantized into regular intervals, and the Y-axis represents the keys of a piano in increasing pitch. A cell in this matrix is $1$ if the key represented by its Y-coordinate is sounding at the time instant represented by its Y-coordinate.
The majority of recent transcription papers utilise and expand \emph{spectrogram factorisation} techniques, such as non-negative matrix factorisation (NMF) \cite{Li1999} and its probabilistic counterpart, probabilistic latent component analysis (PLCA) \cite{Smaragdis2006}. Spectrogram factorisation techniques decompose an input spectrogram of the audio signal into a product of spectral templates (that typically correspond to musical notes) and component activations (that indicate whether each note is active at a given time frame). Spectrogram factorisation-based AMT systems include the work by Bertin et al.\ \cite{Bertin2009}, who proposed a Bayesian framework for NMF, which considers each pitch as a model of Gaussian components in harmonic positions. Benetos and Dixon \cite{Benetos2012} proposed a convolutive model based on PLCA, which supports the transcription of multiple-instrument music and supports tuning changes and frequency modulations (modelled as shifts across log-frequency). 

In terms of connectionist approaches for AMT, Nam et al. \cite{Nam2011} proposed a method where features suitable for transcribing music are learned using a deep belief network consisting of stacked restricted Boltzmann machines (RBMs). The model performed classification using support vector machines and was applied to piano music. B\"{o}ck and Schedl used recurrent neural networks (RNNs) with Long Short-Term Memory units for performing polyphonic piano transcription \cite{Bock2012}, with the system being particularly good at recognising note onsets. 

There is no doubt that a reliable acoustic model is important for generating accurate symbolic transcriptions of a given music signal. However, since music exhibits a fair amount of structural regularity much like language, it is natural for one to think of the possibility of improving transcription accuracy using a \textit{music language model} (MLM) in a manner akin to the use of a language model to improve the performance of a speech recognizer \cite{Rabiner1993}. In \cite{Boulanger-Lewandowski2012}, the predictions of a polyphonic MLM were used to this end, which was further developed in \cite{Boulanger-Lewandowski2013}, where an input/output extension of the RNN-RBM was proposed that learned to map input sequences to output sequences in the context of AMT. Both in \cite{Boulanger-Lewandowski2012} and \cite{Boulanger-Lewandowski2013}, evaluations were performed using synthesized MIDI data. In \cite{Raczynski13}, Raczy\'{n}ski et al. utilise chord and key information for improving an NMF-based AMT system 
in a post-processing step. A major advantage of using a hybrid acoustic + language model system is that the two models can be trained independently using data from different sources. This is particularly useful since annotated audio data is scarce while it is relatively easy to find MIDI data for training robust language models. 

%Another example of symbolic information which can improve the performance of acoustic models are \textit{score informed} approaches, which have been applied in music research tasks, e.g. for tonic identification \cite{Senturk2013}. % More references, if required: Source separation - Ganseman2010, Hennequin2011

In the present work, we integrate a MLM with an AMT system, in order to improve transcription performance. Specifically, we make use of the predictions made by a Recurrent Neural Network (RNN) and a RNN-Neural Autoregressive Distribution Estimator (RNN-NADE) based polyphonic MLM proposed in \cite{Boulanger-Lewandowski2012} to refine the transcriptions of a PLCA-based AMT system \cite{Benetos2012, Benetos2013}. Information from the MLM is incorporated into the PLCA-based acoustic model as a prior for pitch activations during parameter estimation. It is observed that combining the two models in this way boosts transcription accuracy by +3\% on the Bach10 dataset of multiple-instrument polyphonic music \cite{Duan2010}, compared to using the acoustic AMT system only.

The outline of this paper is as follows. The PLCA-based transcription system is presented in Section \ref{sec:transcription}. The RNN-based polyphonic music prediction system that is used as a music language model is described in Section \ref{sec:prediction}. The combination of the two aforementioned systems is presented in Section \ref{sec:combination}. The employed dataset, evaluation metrics, and experimental results are shown in Section \ref{sec:evaluation}; finally, conclusions are drawn in Section \ref{sec:conclusions}.